{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1. \n",
    "\n",
    "Write a Python script to implement the backpropagation algorithm for a 1− $S^{1}$−1 network. \n",
    "\n",
    "Write the program using matrix operations. Choose the initial weights and biases to be random numbers *uniformly* distributed between -0.5 and 0.5, and train the network to approximate the function:\n",
    "\n",
    "$g(p) = e^{-\\vert p \\vert}\\sin(\\pi p)$ for $−2 \\leq p \\leq 2$\n",
    "\n",
    "Use $S^{1}=2$ and $S^{1}=10$. Experiment with several different values for the learning rate, $\\alpha$, and use several different initial conditions. Discuss the convergence properties of the algorithm as the learning rate changes.\n",
    "\n",
    "Plot the trained networks with the network outputs. Compare them. Check the squared error for each epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the transfer functions we need for our network\n",
    "\n",
    "# logsig transfer function\n",
    "def logsig(n):\n",
    "    return 1 / (1 + np.exp(-n))\n",
    "\n",
    "# purelin transfer function\n",
    "def purelin(n):\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the gradients of the transfer functions\n",
    "\n",
    "# logsig transfer function\n",
    "def logsig_grad(a): \n",
    "    return (1 - a) * a\n",
    "\n",
    "# purelin transfer function\n",
    "def purelin_grad(a): \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate 100 linearly spaced inputs in the range [-2, 2]\n",
    "p = \n",
    "# make the inputs 2D so we can specify input dimensions\n",
    "p = p.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute targets by using function to approximate, g(p)\n",
    "g = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the function we want to approximate\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(p, g, label='$g(p)$')\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('$g(p)$')\n",
    "plt.title('Function to approximate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall.** When we have a 3-layer Perceptron, we say it is an $R-S^{1}-S^{2}-S^{3}$ network. Since our network is a $1-S^{1}-1$ network, we know $R=1$ and $S^{2}=1$. We let $S^{1}$ fluctuate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify number of features: R\n",
    "R = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. $S^{1} = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify number of neurons in the hidden layer: S1\n",
    "S1 = \n",
    "# specify number of neurons in the output layer: S2\n",
    "S2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# randomly initialize weights\n",
    "W1 = \n",
    "W2 = \n",
    "# randomly initialize bias\n",
    "b1 = \n",
    "b2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning rate, alpha\n",
    "alpha = 0.025\n",
    "# epochs\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize a zero vector of errors for each output-target\n",
    "\n",
    "# error vector for each input\n",
    "e = np.zeros(len(g))\n",
    "# error vector for MSE in each epoch\n",
    "MSE = np.zeros(shape=(1,epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------- Train the network -----------\n",
    "\n",
    "# for each epoch \n",
    "for epoch in range(epochs):\n",
    "    # for each input\n",
    "    for i in range(p.shape[0]):\n",
    "        # 1) propagate input forward\n",
    "        n1 = \n",
    "        a1 = \n",
    "        n2 = \n",
    "        a2 = \n",
    "        # 2) calculate the error\n",
    "        # difference between g(p_{i}) and a2 (network output)\n",
    "        e[i] = g[i] - a2\n",
    "        # 3) Backprop sensitivities\n",
    "        g_logsig = \n",
    "        f1_dot = np.diagflat(g_logsig)\n",
    "        f2_dot = \n",
    "        s2 = \n",
    "        s1 = \n",
    "        # 4) Update weight and bias\n",
    "        W2 = \n",
    "        W1 = \n",
    "        b2 = \n",
    "        b1 = \n",
    "    MSE[0, epoch] = e.transpose().dot(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the mean square error (MSE)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.arange(1,epochs+1), MSE[0,:], ls='-')\n",
    "plt.title('Mean Square Error by Epoch, S = 2')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# empty vector for trained network output\n",
    "a_S2 = np.ones(shape=(p.shape))\n",
    "\n",
    "# calculate predicted outputs from the trained network\n",
    "for i in range(p.shape[0]):\n",
    "    n1 = \n",
    "    a1 = \n",
    "    n2 = \n",
    "    a_S2[i] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(p, g, label='Actual', color='green')\n",
    "plt.plot(p, a_S2, label='Predicted', color='blue')\n",
    "plt.title(\"Network Approximation, S = 2\")\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"g(p)\")\n",
    "plt.xlim((-2,2))\n",
    "plt.ylim((-0.67, 0.67))\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. $S^{1} = 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of neurons in the hidden layer\n",
    "S1 = \n",
    "# number of neurons in the output layer\n",
    "S2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# randomly initialize weights\n",
    "W1 = \n",
    "# randomly initialize bias\n",
    "b1 = \n",
    "W2 = \n",
    "b2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize a zero vector of errors for each output-target\n",
    "\n",
    "# error vector for each input\n",
    "e = np.zeros(len(g))\n",
    "# error vector for MSE in each epoch\n",
    "MSE = np.zeros(shape=(1,epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------- Train the network -----------\n",
    "\n",
    "# for each epoch \n",
    "for epoch in range(epochs):\n",
    "    # for each input\n",
    "    for i in range(p.shape[0]):\n",
    "        # 1) propagate input forward\n",
    "        n1 = \n",
    "        a1 = \n",
    "        n2 = \n",
    "        a2 = \n",
    "        # 2) calculate the error\n",
    "        # difference between g(p_{i}) and a2 (network output)\n",
    "        e[i] = \n",
    "        # 3) Backprop sensitivities\n",
    "        g_logsig = \n",
    "        f1_dot = \n",
    "        f2_dot = \n",
    "        s2 = \n",
    "        s1 = \n",
    "        # 4) Update weight and bias\n",
    "        W2 = \n",
    "        W1 = \n",
    "        b2 = \n",
    "        b1 = \n",
    "    MSE[0, epoch] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(np.arange(1,epochs+1), MSE[0,:], ls='-')\n",
    "plt.title('Mean Square Error by Epoch, S = 10')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# empty vector for trained network output\n",
    "a_S10 = np.ones(shape=(p.shape))\n",
    "\n",
    "# calculate network output\n",
    "for i in range(p.shape[0]):\n",
    "    n1 = \n",
    "    a1 = \n",
    "    n2 = \n",
    "    a_S10[i] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(p, g, label='Actual', color='green')\n",
    "plt.plot(p, a_S2, label='1-S2-1 Network', color='blue')\n",
    "plt.plot(p, a_S10, label='1-S10-1 Network', color='red')\n",
    "plt.title(\"Network Approximation, S = 10\")\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"g(p)\")\n",
    "plt.xlim((-2,2))\n",
    "plt.ylim((-0.67, 0.67))\n",
    "#plt.tight_layout()\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
